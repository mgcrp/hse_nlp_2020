{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Toxic Spans Detection Advanced Model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Wgo20ZvSkuc5",
        "M8iVa1nPjEX1",
        "qDZK4t5AkRdw",
        "km7xNJpnk1oU",
        "6yU3Wy3gEKUC",
        "We0VisBiEVuL",
        "gw1UtUqeFiIV"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01c3f86a23c34f2a9cd2b58cc44154d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_168542a6a6f6488ead299f5b7eb0247a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ee18733eefcc44659cf885ac90996d73",
              "IPY_MODEL_fead42ad98974c4281cb1edb5c4bafaf"
            ]
          }
        },
        "168542a6a6f6488ead299f5b7eb0247a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ee18733eefcc44659cf885ac90996d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_741a72c1c846482fbb56b1b411834793",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_958010b3d731490299cbd9f6fe0b0e31"
          }
        },
        "fead42ad98974c4281cb1edb5c4bafaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_378da732cae44c489bc537d6c5f6348b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:00&lt;00:00, 707kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d7f65baf04644006bd44edbc009065f2"
          }
        },
        "741a72c1c846482fbb56b1b411834793": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "958010b3d731490299cbd9f6fe0b0e31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "378da732cae44c489bc537d6c5f6348b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d7f65baf04644006bd44edbc009065f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6ee592945375443397d923a472b71989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8551949750b14a909cab9a17aac15efb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_797694c8aeee4155b486365eb9465070",
              "IPY_MODEL_d732410c46534276ac937bb10906fab0"
            ]
          }
        },
        "8551949750b14a909cab9a17aac15efb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "797694c8aeee4155b486365eb9465070": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9f9e0639067b4c27a4dd156fd6152f7d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 625,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 625,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7af8cf7b19b9416ba79b87457d6b8b90"
          }
        },
        "d732410c46534276ac937bb10906fab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_54bb45cfc33348f48a4eed60c39dfa14",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 625/625 [00:57&lt;00:00, 10.9B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_efb02a734c1d4483aa4efdb6ae770178"
          }
        },
        "9f9e0639067b4c27a4dd156fd6152f7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7af8cf7b19b9416ba79b87457d6b8b90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "54bb45cfc33348f48a4eed60c39dfa14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "efb02a734c1d4483aa4efdb6ae770178": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b2188b5a67b04b7bbe9df91a3603755d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_270cd69c2e2d43deab02682a1d979eb3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a179c9df0450469097236bce65a6f7a1",
              "IPY_MODEL_2eca0a90d54c433b84f56ec4fab7064f"
            ]
          }
        },
        "270cd69c2e2d43deab02682a1d979eb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a179c9df0450469097236bce65a6f7a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b68be9c566024d959dd3d1c01bb1e347",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1338740706,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1338740706,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab01b75f80af448ea39e6d27af5d43a5"
          }
        },
        "2eca0a90d54c433b84f56ec4fab7064f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_86181ab2d34c4b6a848cbd8b940939d8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.34G/1.34G [00:46&lt;00:00, 28.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7b4d690569bb4d99b036dcd744aa6d11"
          }
        },
        "b68be9c566024d959dd3d1c01bb1e347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab01b75f80af448ea39e6d27af5d43a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "86181ab2d34c4b6a848cbd8b940939d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7b4d690569bb4d99b036dcd744aa6d11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "c205N2_pxSXE"
      },
      "source": [
        "pip install transformers==2.6.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aPO3uDbKx65"
      },
      "source": [
        "pip install seqeval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCGWM4fsbkXt"
      },
      "source": [
        "pip install fasttext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ui8fuL9b7We"
      },
      "source": [
        "# Baseline Model: Embedding + BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FOh1QtTcIIa"
      },
      "source": [
        "import fasttext\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras_preprocessing import sequence\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwYsct1YeE37"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "O0Ku0-UtcRB3",
        "outputId": "75587e1c-fbb2-40ea-e843-7be8e11a1721"
      },
      "source": [
        "df = pd.read_csv('tsd_train.csv')\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spans</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,...</td>\n",
              "      <td>Another violent and aggressive immigrant killi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[33, 34, 35, 36, 37, 38, 39]</td>\n",
              "      <td>I am 56 years old, I am not your fucking junio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0, 1, 2, 3]</td>\n",
              "      <td>Damn, a whole family. Sad indeed.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]</td>\n",
              "      <td>What a knucklehead. How can anyone not know th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[32, 33, 34, 35, 36, 37, 38]</td>\n",
              "      <td>\"who do you think should do the killing?\"\\n\\nA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[]</td>\n",
              "      <td>But, but, but, is NOT a defense.  It's not eve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 5...</td>\n",
              "      <td>Please people, stop using these silly, stupid ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[0, 1, 2, 3]</td>\n",
              "      <td>Dumb.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[49, 50, 51, 52, 53, 54, 147, 148, 149, 150, 1...</td>\n",
              "      <td>Obamacare is on it's last gasping breaths.   Y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[32, 33, 34, 35, 36, 37, 38, 39]</td>\n",
              "      <td>CROOKED Trump = GUILTY as hell.\\npathetic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               spans                                               text\n",
              "0  [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,...  Another violent and aggressive immigrant killi...\n",
              "1                       [33, 34, 35, 36, 37, 38, 39]  I am 56 years old, I am not your fucking junio...\n",
              "2                                       [0, 1, 2, 3]                  Damn, a whole family. Sad indeed.\n",
              "3          [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]  What a knucklehead. How can anyone not know th...\n",
              "4                       [32, 33, 34, 35, 36, 37, 38]  \"who do you think should do the killing?\"\\n\\nA...\n",
              "5                                                 []  But, but, but, is NOT a defense.  It's not eve...\n",
              "6  [39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 5...  Please people, stop using these silly, stupid ...\n",
              "7                                       [0, 1, 2, 3]                                              Dumb.\n",
              "8  [49, 50, 51, 52, 53, 54, 147, 148, 149, 150, 1...  Obamacare is on it's last gasping breaths.   Y...\n",
              "9                   [32, 33, 34, 35, 36, 37, 38, 39]          CROOKED Trump = GUILTY as hell.\\npathetic"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Th9Rs7ecT-Z"
      },
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def list_replace(search, replacement, text):\n",
        "    \"\"\"\n",
        "    Replaces all symbols of text which are present\n",
        "    in the search string with the replacement string.\n",
        "    \"\"\"\n",
        "    search = [el for el in search if el in text]\n",
        "    for c in search:\n",
        "        text = text.replace(c, replacement)\n",
        "    return text\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = list_replace('\\u00AB\\u00BB\\u2039\\u203A\\u201E\\u201A\\u201C\\u201F\\u2018\\u201B\\u201D\\u2019', '\\u0022', text)\n",
        "    text = list_replace('\\u2012\\u2013\\u2014\\u2015\\u203E\\u0305\\u00AF', '\\u2003\\u002D\\u002D\\u2003', text)\n",
        "    text = list_replace('\\u2010\\u2011', '\\u002D', text)\n",
        "    text = list_replace('\\u2000\\u2001\\u2002\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200A\\u200B\\u202F\\u205F\\u2060\\u3000', '\\u2002', text)\n",
        "    text = re.sub('\\u2003\\u2003', '\\u2003', text)\n",
        "    text = re.sub('\\t\\t', '\\t', text)\n",
        "    text = list_replace('\\u02CC\\u0307\\u0323\\u2022\\u2023\\u2043\\u204C\\u204D\\u2219\\u25E6\\u00B7\\u00D7\\u22C5\\u2219\\u2062', '.', text)\n",
        "    text = list_replace('\\u2217', '\\u002A', text)\n",
        "\n",
        "    text = list_replace('\\u00C4', 'A', text)\n",
        "    text = list_replace('\\u00E4', 'a', text)\n",
        "    text = list_replace('\\u00CB', 'E', text)\n",
        "    text = list_replace('\\u00EB', 'e', text)\n",
        "    text = list_replace('\\u1E26', 'H', text)\n",
        "    text = list_replace('\\u1E27', 'h', text)\n",
        "    text = list_replace('\\u00CF', 'I', text)\n",
        "    text = list_replace('\\u00EF', 'i', text)\n",
        "    text = list_replace('\\u00D6', 'O', text)\n",
        "    text = list_replace('\\u00F6', 'o', text)\n",
        "    text = list_replace('\\u00DC', 'U', text)\n",
        "    text = list_replace('\\u00FC', 'u', text)\n",
        "    text = list_replace('\\u0178', 'Y', text)\n",
        "    text = list_replace('\\u00FF', 'y', text)\n",
        "    text = list_replace('\\u00DF', 's', text)\n",
        "    text = list_replace('\\u1E9E', 'S', text)\n",
        "\n",
        "    # Removing punctuation\n",
        "    text = list_replace(',.[]{}()=+-−*&^%$#@!~;:§/\\|\\?\"\\n', ' ', text)\n",
        "    \n",
        "    # Replacing all numbers with masks\n",
        "    text = list_replace('0123456789', 'x', text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbzKUsUdcouI"
      },
      "source": [
        "# Переходим от символов к словам\n",
        "\n",
        "X = [[]]\n",
        "y = [[]]\n",
        "for i in range(df.shape[0]):\n",
        "    cl_text = clean_text(df.iloc[i,1])\n",
        "    fl = np.zeros(len(cl_text))\n",
        "    if(len(df.iloc[i,0]) > 2):\n",
        "        arr = [int(a) for a in df.iloc[i,0][1:-1].split(', ')]\n",
        "        fl[arr] = 1\n",
        "    spl = cl_text.split(' ')\n",
        "    prev_len = 0\n",
        "    X_i = []\n",
        "    y_i = []\n",
        "    for j in range(len(spl)):\n",
        "        if(len(spl[j]) > 0):\n",
        "            X_i.append(spl[j])\n",
        "            y_i.append(sum(fl[prev_len:prev_len+len(spl[j])])/len(spl[j]))\n",
        "        prev_len = prev_len + len(spl[j]) + 1\n",
        "    X.append(X_i)\n",
        "    y.append(y_i)\n",
        "X = X[1:]\n",
        "y = y[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yccLmwd4ctFc",
        "outputId": "cd2aa3ae-ff7b-4009-c604-220c69523a0a"
      },
      "source": [
        "cleaned_train_texts = [' '.join(i) for i in X]\n",
        "maxlen = max([len(i) for i in X])\n",
        "maxlen"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "201"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMAajp-dcx2C",
        "outputId": "658e9099-4e51-4b44-830d-331f14cb7f9d"
      },
      "source": [
        "! wget -O model_fasttext.bin https://www.dropbox.com/s/f8svib34687gyb4/rudrec_fasttext_model.bin?dl=0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-20 00:01:14--  https://www.dropbox.com/s/f8svib34687gyb4/rudrec_fasttext_model.bin?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6022:18::a27d:4212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/f8svib34687gyb4/rudrec_fasttext_model.bin [following]\n",
            "--2020-12-20 00:01:15--  https://www.dropbox.com/s/raw/f8svib34687gyb4/rudrec_fasttext_model.bin\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc149538eae897249b2e1ec1563e.dl.dropboxusercontent.com/cd/0/inline/BFaJGav_pdj6KOo1ne1A8EpcHG1DrSM2jOyhzsbuGAHrQXOxpf8iYXFctNDms6MtuqR0Cme_wnZ26Qu2PQMRrpkQ0HCyK_iS0249qjDC_abk_Yu__aFs4IG27Rt9Rtrwrug/file# [following]\n",
            "--2020-12-20 00:01:15--  https://uc149538eae897249b2e1ec1563e.dl.dropboxusercontent.com/cd/0/inline/BFaJGav_pdj6KOo1ne1A8EpcHG1DrSM2jOyhzsbuGAHrQXOxpf8iYXFctNDms6MtuqR0Cme_wnZ26Qu2PQMRrpkQ0HCyK_iS0249qjDC_abk_Yu__aFs4IG27Rt9Rtrwrug/file\n",
            "Resolving uc149538eae897249b2e1ec1563e.dl.dropboxusercontent.com (uc149538eae897249b2e1ec1563e.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6022:15::a27d:420f\n",
            "Connecting to uc149538eae897249b2e1ec1563e.dl.dropboxusercontent.com (uc149538eae897249b2e1ec1563e.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BFb7kDsaYkiTLwkKHcE-KzNpcq52kkJmDcQ9XEwDejuvbq44d5aV54cLg4_s4vImqkrp2-dEwpxPpJe4XYARGpqAKd7-jpieGgAoEXGRKPzM8IZ8treliFxZS2OFbNGp2TO5t1gx_NzxZv3pJvOODEtdQqVyIB5N_rNnBLUF12pCwro9HQ6QtPnl8ucvX1dZZvEHF7_zhk9vK12cAX7DDabhcKiQdzbIoanO1rn62koT7cOwS_dO9dHEWbIzTB71CbSNLUfwQCIZcsarIeNBJATbCGU1zrPSCGX64-Lgj3N3i2ILBHeMmVNXunB6D0EYcJx3ox7ZWXl7vR9cKlVXS6shgybOYRZyWnxpr4sF2lVFoQ/file [following]\n",
            "--2020-12-20 00:01:16--  https://uc149538eae897249b2e1ec1563e.dl.dropboxusercontent.com/cd/0/inline2/BFb7kDsaYkiTLwkKHcE-KzNpcq52kkJmDcQ9XEwDejuvbq44d5aV54cLg4_s4vImqkrp2-dEwpxPpJe4XYARGpqAKd7-jpieGgAoEXGRKPzM8IZ8treliFxZS2OFbNGp2TO5t1gx_NzxZv3pJvOODEtdQqVyIB5N_rNnBLUF12pCwro9HQ6QtPnl8ucvX1dZZvEHF7_zhk9vK12cAX7DDabhcKiQdzbIoanO1rn62koT7cOwS_dO9dHEWbIzTB71CbSNLUfwQCIZcsarIeNBJATbCGU1zrPSCGX64-Lgj3N3i2ILBHeMmVNXunB6D0EYcJx3ox7ZWXl7vR9cKlVXS6shgybOYRZyWnxpr4sF2lVFoQ/file\n",
            "Reusing existing connection to uc149538eae897249b2e1ec1563e.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2048104027 (1.9G) [application/octet-stream]\n",
            "Saving to: ‘model_fasttext.bin’\n",
            "\n",
            "model_fasttext.bin  100%[===================>]   1.91G  12.9MB/s    in 99s     \n",
            "\n",
            "2020-12-20 00:02:56 (19.7 MB/s) - ‘model_fasttext.bin’ saved [2048104027/2048104027]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q63_AFdKc0JI",
        "outputId": "ab1c2542-d7b7-486d-a7f3-8198cca9c8db"
      },
      "source": [
        "# Загружаем предобученную fasttext модель\n",
        "\n",
        "fasttext_model_path = 'model_fasttext.bin'\n",
        "fasttext_model = fasttext.load_model(fasttext_model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWSgijQ2c_Am"
      },
      "source": [
        "# Получаем эмбеддинги\n",
        "\n",
        "EMBEDDINGS_DIM = 200\n",
        "tokenizer = Tokenizer(lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(cleaned_train_texts)\n",
        "word_seq_train = tokenizer.texts_to_sequences(cleaned_train_texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "trash = []\n",
        "for i in range(len(X)):\n",
        "    if(len(X[i]) != len(word_seq_train[i])):\n",
        "        trash.append(i)\n",
        "for i in trash:\n",
        "    word_seq_train.pop(i)\n",
        "    X.pop(i)\n",
        "    y.pop(i)\n",
        "  \n",
        "# Дополняем нулями до длины maxlen\n",
        "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=maxlen, padding=\"post\")\n",
        "\n",
        "dictionary_size = len(word_index.keys())\n",
        "embedding_matrix = np.zeros((dictionary_size + 1, EMBEDDINGS_DIM))\\\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = fasttext_model.get_word_vector((word))\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XguP8vBodCkm"
      },
      "source": [
        "# Дополняем нулями до длины maxlen\n",
        "\n",
        "y = sequence.pad_sequences(y, maxlen=maxlen, padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x8Hz_HqdHSY",
        "outputId": "abdd3b5a-4211-4035-cf47-d5b5e7bc24b9"
      },
      "source": [
        "from keras.models import Model, Input\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "\n",
        "# Модель со слоем эмбеддингов, двунаправленным LSTM и полносвязным слоем к каждому из выходов BiLSTM\n",
        "input = Input(shape=(maxlen,))\n",
        "out = Embedding(len(word_index.keys())+1, 200, input_length=maxlen, weights=[embedding_matrix])(input)\n",
        "out = Dropout(0.1)(out)\n",
        "out = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(out)\n",
        "out = TimeDistributed(Dense(1, activation=\"sigmoid\"))(out)\n",
        "model = Model(input, out)\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbI36I53dMuO"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Разбиваем выборку на обучающую и тестовую и обучаем\n",
        "X_train, X_test, y_train, y_test = train_test_split(word_seq_train, y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYNx3RJydVur",
        "outputId": "1bc54a15-0836-48ff-a208-2041083d86fc"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=32, epochs=3, validation_split=0.1, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "178/178 [==============================] - 213s 1s/step - loss: 0.1587 - accuracy: 0.9811 - val_loss: 0.0454 - val_accuracy: 0.9858\n",
            "Epoch 2/3\n",
            "178/178 [==============================] - 207s 1s/step - loss: 0.0396 - accuracy: 0.9878 - val_loss: 0.0428 - val_accuracy: 0.9873\n",
            "Epoch 3/3\n",
            "178/178 [==============================] - 206s 1s/step - loss: 0.0353 - accuracy: 0.9890 - val_loss: 0.0434 - val_accuracy: 0.9873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8b7e9057b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4ck9FV8dX8H"
      },
      "source": [
        "pr = model.predict(X_test).reshape(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_5Ag6YbdbBB",
        "outputId": "dfdefbf1-99d4-4c5e-f94c-7f034d92aa20"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "np.mean(tf.keras.losses.binary_crossentropy(y_test, pr.reshape(y_test.shape)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.045947444"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73Fl0-hzddZQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Приводим выход к нужному формату для корректного подсчёта f1-score\n",
        "arr1 = []\n",
        "arr2 = []\n",
        "for i in range(X_test.shape[0]):\n",
        "    l = len(X_test[i])\n",
        "    arr1.append([round(i) for i in list(pr[i][:l])])\n",
        "    arr2.append(list(y_test[i][:l]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiS7cc_wdi4F",
        "outputId": "6d013b4c-32c8-41f0-c6f9-2a6984fc61c2"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "print(f'Итоговый скор: {np.mean([f1_score(arr2[i], arr1[i]) for i in range(len(arr2))])}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Итоговый скор: 0.577663187266675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrGulKRFuTMA"
      },
      "source": [
        "# BERT Base Cased Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wgo20ZvSkuc5"
      },
      "source": [
        "## Init CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "AZEQI2K0uSyB",
        "outputId": "9c2621b8-261b-4528-afd5-da142327ec2e"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.7.0+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "UlsNt16WcLuc",
        "outputId": "f3e2a2a0-89b6-46e0-9c5e-72a3724d251b"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla V100-SXM2-16GB'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCKyubPwmTjH"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8iVa1nPjEX1"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "94TBo2UvGufG",
        "outputId": "cfbf16fc-0528-469f-a0b3-ce1460a0cf2a"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('tsd_train.csv')\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spans</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,...</td>\n",
              "      <td>Another violent and aggressive immigrant killi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[33, 34, 35, 36, 37, 38, 39]</td>\n",
              "      <td>I am 56 years old, I am not your fucking junio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0, 1, 2, 3]</td>\n",
              "      <td>Damn, a whole family. Sad indeed.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]</td>\n",
              "      <td>What a knucklehead. How can anyone not know th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[32, 33, 34, 35, 36, 37, 38]</td>\n",
              "      <td>\"who do you think should do the killing?\"\\n\\nA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[]</td>\n",
              "      <td>But, but, but, is NOT a defense.  It's not eve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 5...</td>\n",
              "      <td>Please people, stop using these silly, stupid ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[0, 1, 2, 3]</td>\n",
              "      <td>Dumb.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[49, 50, 51, 52, 53, 54, 147, 148, 149, 150, 1...</td>\n",
              "      <td>Obamacare is on it's last gasping breaths.   Y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[32, 33, 34, 35, 36, 37, 38, 39]</td>\n",
              "      <td>CROOKED Trump = GUILTY as hell.\\npathetic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               spans                                               text\n",
              "0  [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,...  Another violent and aggressive immigrant killi...\n",
              "1                       [33, 34, 35, 36, 37, 38, 39]  I am 56 years old, I am not your fucking junio...\n",
              "2                                       [0, 1, 2, 3]                  Damn, a whole family. Sad indeed.\n",
              "3          [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]  What a knucklehead. How can anyone not know th...\n",
              "4                       [32, 33, 34, 35, 36, 37, 38]  \"who do you think should do the killing?\"\\n\\nA...\n",
              "5                                                 []  But, but, but, is NOT a defense.  It's not eve...\n",
              "6  [39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 5...  Please people, stop using these silly, stupid ...\n",
              "7                                       [0, 1, 2, 3]                                              Dumb.\n",
              "8  [49, 50, 51, 52, 53, 54, 147, 148, 149, 150, 1...  Obamacare is on it's last gasping breaths.   Y...\n",
              "9                   [32, 33, 34, 35, 36, 37, 38, 39]          CROOKED Trump = GUILTY as hell.\\npathetic"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpM1SSMZK0hP"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEjT14GnN-ZY"
      },
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def list_replace(search, replacement, text):\n",
        "    \"\"\"\n",
        "    Replaces all symbols of text which are present\n",
        "    in the search string with the replacement string.\n",
        "    \"\"\"\n",
        "    search = [el for el in search if el in text]\n",
        "    for c in search:\n",
        "        text = text.replace(c, replacement)\n",
        "    return text\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans the given sentence\n",
        "    \"\"\"\n",
        "    text = list_replace('\\u00AB\\u00BB\\u2039\\u203A\\u201E\\u201A\\u201C\\u201F\\u2018\\u201B\\u201D\\u2019', '\\u0022', text)\n",
        "    text = list_replace('\\u2012\\u2013\\u2014\\u2015\\u203E\\u0305\\u00AF', '\\u2003\\u002D\\u002D\\u2003', text)\n",
        "    text = list_replace('\\u2010\\u2011', '\\u002D', text)\n",
        "    text = list_replace('\\u2000\\u2001\\u2002\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200A\\u200B\\u202F\\u205F\\u2060\\u3000', '\\u2002', text)\n",
        "    text = re.sub('\\u2003\\u2003', '\\u2003', text)\n",
        "    text = re.sub('\\t\\t', '\\t', text)\n",
        "    text = list_replace('\\u02CC\\u0307\\u0323\\u2022\\u2023\\u2043\\u204C\\u204D\\u2219\\u25E6\\u00B7\\u00D7\\u22C5\\u2219\\u2062', '.', text)\n",
        "    text = list_replace('\\u2217', '\\u002A', text)\n",
        "    text = list_replace('\\u00C4', 'A', text)\n",
        "    text = list_replace('\\u00E4', 'a', text)\n",
        "    text = list_replace('\\u00CB', 'E', text)\n",
        "    text = list_replace('\\u00EB', 'e', text)\n",
        "    text = list_replace('\\u1E26', 'H', text)\n",
        "    text = list_replace('\\u1E27', 'h', text)\n",
        "    text = list_replace('\\u00CF', 'I', text)\n",
        "    text = list_replace('\\u00EF', 'i', text)\n",
        "    text = list_replace('\\u00D6', 'O', text)\n",
        "    text = list_replace('\\u00F6', 'o', text)\n",
        "    text = list_replace('\\u00DC', 'U', text)\n",
        "    text = list_replace('\\u00FC', 'u', text)\n",
        "    text = list_replace('\\u0178', 'Y', text)\n",
        "    text = list_replace('\\u00FF', 'y', text)\n",
        "    text = list_replace('\\u00DF', 's', text)\n",
        "    text = list_replace('\\u1E9E', 'S', text)\n",
        "\n",
        "    # Replacing all numbers with masks\n",
        "    text = list_replace('0123456789', 'x', text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "w-2bFwsmOrSd",
        "outputId": "05f7fb0b-9d6f-4d5b-c3f4-a742417d4955"
      },
      "source": [
        "df['text_normalized'] = df.text.apply(clean_text)\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spans</th>\n",
              "      <th>text</th>\n",
              "      <th>text_normalized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,...</td>\n",
              "      <td>Another violent and aggressive immigrant killi...</td>\n",
              "      <td>Another violent and aggressive immigrant killi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[33, 34, 35, 36, 37, 38, 39]</td>\n",
              "      <td>I am 56 years old, I am not your fucking junio...</td>\n",
              "      <td>I am xx years old, I am not your fucking junio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0, 1, 2, 3]</td>\n",
              "      <td>Damn, a whole family. Sad indeed.</td>\n",
              "      <td>Damn, a whole family. Sad indeed.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]</td>\n",
              "      <td>What a knucklehead. How can anyone not know th...</td>\n",
              "      <td>What a knucklehead. How can anyone not know th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[32, 33, 34, 35, 36, 37, 38]</td>\n",
              "      <td>\"who do you think should do the killing?\"\\n\\nA...</td>\n",
              "      <td>\"who do you think should do the killing?\"\\n\\nA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[]</td>\n",
              "      <td>But, but, but, is NOT a defense.  It's not eve...</td>\n",
              "      <td>But, but, but, is NOT a defense.  It's not eve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 5...</td>\n",
              "      <td>Please people, stop using these silly, stupid ...</td>\n",
              "      <td>Please people, stop using these silly, stupid ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[0, 1, 2, 3]</td>\n",
              "      <td>Dumb.</td>\n",
              "      <td>Dumb.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[49, 50, 51, 52, 53, 54, 147, 148, 149, 150, 1...</td>\n",
              "      <td>Obamacare is on it's last gasping breaths.   Y...</td>\n",
              "      <td>Obamacare is on it's last gasping breaths.   Y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[32, 33, 34, 35, 36, 37, 38, 39]</td>\n",
              "      <td>CROOKED Trump = GUILTY as hell.\\npathetic</td>\n",
              "      <td>CROOKED Trump = GUILTY as hell.\\npathetic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               spans  ...                                    text_normalized\n",
              "0  [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,...  ...  Another violent and aggressive immigrant killi...\n",
              "1                       [33, 34, 35, 36, 37, 38, 39]  ...  I am xx years old, I am not your fucking junio...\n",
              "2                                       [0, 1, 2, 3]  ...                  Damn, a whole family. Sad indeed.\n",
              "3          [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]  ...  What a knucklehead. How can anyone not know th...\n",
              "4                       [32, 33, 34, 35, 36, 37, 38]  ...  \"who do you think should do the killing?\"\\n\\nA...\n",
              "5                                                 []  ...  But, but, but, is NOT a defense.  It's not eve...\n",
              "6  [39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 5...  ...  Please people, stop using these silly, stupid ...\n",
              "7                                       [0, 1, 2, 3]  ...                                              Dumb.\n",
              "8  [49, 50, 51, 52, 53, 54, 147, 148, 149, 150, 1...  ...  Obamacare is on it's last gasping breaths.   Y...\n",
              "9                   [32, 33, 34, 35, 36, 37, 38, 39]  ...          CROOKED Trump = GUILTY as hell.\\npathetic\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqZ4sXZUQei-"
      },
      "source": [
        "def create_labels(label_chars, text, embed, debug=False):\n",
        "    \"\"\"\n",
        "    Creates tokens and labels for BERT to work with\n",
        "    \"\"\"\n",
        "    res_tokens = []\n",
        "    res_labels = []\n",
        "\n",
        "    res_tokens.append('[CLS]')\n",
        "    res_labels.append('O')\n",
        "\n",
        "    if label_chars == '[]':\n",
        "        for i in embed:\n",
        "            res_tokens.append(i)\n",
        "            res_labels.append('O')\n",
        "    else:\n",
        "        _last = -1\n",
        "        _first = -1\n",
        "        arr = [int(a) for a in label_chars[1:-1].split(', ')]\n",
        "\n",
        "        bad_tokens = []\n",
        "        \n",
        "        for i in range(len(arr)):\n",
        "            if i == 0:\n",
        "                _first = arr[i]\n",
        "            elif arr[i-1] + 1 != arr[i]:\n",
        "                _last = arr[i-1]\n",
        "                _word = tokenizer.tokenize(text[_first:_last+1])\n",
        "                if debug: print(arr, _word, text)\n",
        "                bad_tokens.extend(_word)\n",
        "                _first = arr[i]\n",
        "            elif i == len(arr)-1:\n",
        "                _last = arr[i]\n",
        "                _word = tokenizer.tokenize(text[_first:_last+1])\n",
        "                if debug: print(arr, _word, text)\n",
        "                bad_tokens.extend(_word)\n",
        "        \n",
        "        j = 0\n",
        "        for i in embed:\n",
        "            if j < len(bad_tokens) and i == bad_tokens[j]:\n",
        "                j += 1\n",
        "                res_tokens.append(i)\n",
        "                res_labels.append('I-OFF')\n",
        "            else:\n",
        "                res_tokens.append(i)\n",
        "                res_labels.append('O')\n",
        "\n",
        "    res_tokens.append('[SEP]')\n",
        "    res_labels.append('O')\n",
        "\n",
        "    flag_isFirst = True\n",
        "    for i in range(len(res_labels)):\n",
        "        if res_labels[i] == 'I-OFF' and flag_isFirst:\n",
        "            res_labels[i] = 'B-OFF'\n",
        "            flag_isFirst = False\n",
        "        elif res_labels[i] == 'O':\n",
        "            flag_isFirst = True\n",
        "\n",
        "    if debug:\n",
        "        for i, j in zip(res_tokens, res_labels):\n",
        "            print(f'{i}\\t\\t\\t{j}')\n",
        "\n",
        "    return res_tokens, res_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUNSxGf2TsUG"
      },
      "source": [
        "tokenized_texts_and_labels = [\n",
        "    create_labels(span, text, tokenizer.tokenize(norm)) for span, text, norm in zip(df.spans.tolist(), df.text.tolist(), df.text_normalized.tolist())\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbKu5e4XaW8W"
      },
      "source": [
        "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
        "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypu2kkM4a0ua",
        "outputId": "eda7f961-6250-4f45-9677-582928f2aa55"
      },
      "source": [
        "print(f'Длина самого большого embeding: {max(list(map(len, tokenized_texts)))}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Длина самого большого embeding: 501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym7y9s2Qa3zM"
      },
      "source": [
        "MAX_LEN = 510\n",
        "BATCH_SIZE = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MWNXgssa6S0"
      },
      "source": [
        "input_ids = pad_sequences(\n",
        "    [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "    maxlen=MAX_LEN,\n",
        "    dtype=\"long\",\n",
        "    value=0.0,\n",
        "    truncating=\"post\",\n",
        "    padding=\"post\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFU3ne88beNG"
      },
      "source": [
        "tag_values = ['O', '[PAD]', 'B-OFF', 'I-OFF']\n",
        "tag2idx = {t: i for i, t in enumerate(tag_values)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-jxccava8-x"
      },
      "source": [
        "tags = pad_sequences(\n",
        "    [[tag2idx.get(l) for l in lab] for lab in labels],\n",
        "    maxlen=MAX_LEN,\n",
        "    value=tag2idx[\"[PAD]\"],\n",
        "    padding=\"post\",\n",
        "    dtype=\"long\",\n",
        "    truncating=\"post\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9AxpyBca_oO"
      },
      "source": [
        "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A131BB_XbsDM"
      },
      "source": [
        "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, random_state=42, test_size=0.1)\n",
        "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=42, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPHW7zdybv4c"
      },
      "source": [
        "tr_inputs = torch.tensor(tr_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "tr_tags = torch.tensor(tr_tags)\n",
        "val_tags = torch.tensor(val_tags)\n",
        "tr_masks = torch.tensor(tr_masks)\n",
        "val_masks = torch.tensor(val_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUgyMD99bxvt"
      },
      "source": [
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDZK4t5AkRdw"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "u3lBvU-xbzj5",
        "outputId": "065bc530-64e6-4025-a5ae-2a91be173602"
      },
      "source": [
        "import numpy as np\n",
        "import transformers\n",
        "\n",
        "from tqdm import trange, tqdm\n",
        "from seqeval.metrics import f1_score, accuracy_score\n",
        "from transformers import BertForTokenClassification, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "transformers.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3IXIeC2b1Q-"
      },
      "source": [
        "model = BertForTokenClassification.from_pretrained(\n",
        "    \"bert-base-cased\",\n",
        "    num_labels=len(tag2idx),\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWRN3JTvb28J"
      },
      "source": [
        "model.cuda();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXiQ_zwCb7kL"
      },
      "source": [
        "FULL_FINETUNING = True\n",
        "\n",
        "if FULL_FINETUNING:\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "            'weight_decay_rate': 0.01\n",
        "        },\n",
        "        {\n",
        "            'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "            'weight_decay_rate': 0.0\n",
        "        }\n",
        "    ]\n",
        "else:\n",
        "    param_optimizer = list(model.classifier.named_parameters())\n",
        "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters,\n",
        "    lr=3e-5,\n",
        "    eps=1e-8\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOQoPJU8b_bn"
      },
      "source": [
        "EPOCHS_NUM = 3\n",
        "MAX_GRAD_NORM = 1.0\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * EPOCHS_NUM\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuwQ4sU6cBmQ",
        "outputId": "667fefce-b86f-4d5b-9b9a-0aaf3a3d0a97"
      },
      "source": [
        "loss_values, validation_loss_values = [], []\n",
        "\n",
        "for _ in trange(EPOCHS_NUM, desc=\"Epoch\"):\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Training loop\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # Always clear any previously calculated gradients before performing a backward pass.\n",
        "        model.zero_grad()\n",
        "        # forward pass\n",
        "        # This will return the loss (rather than the model output)\n",
        "        # because we have provided the `labels`.\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        # get the loss\n",
        "        loss = outputs[0]\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        # track train loss\n",
        "        total_loss += loss.item()\n",
        "        # Clip the norm of the gradient\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_GRAD_NORM)\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    # Put the model into evaluation mode\n",
        "    model.eval()\n",
        "    # Reset the validation loss for this epoch.\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions , true_labels = [], []\n",
        "    for batch in valid_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients,\n",
        "        # saving memory and speeding up validation\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have not provided labels.\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        # Move logits and labels to CPU\n",
        "        logits = outputs[1].detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        eval_loss += outputs[0].mean().item()\n",
        "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "        true_labels.extend(label_ids)\n",
        "\n",
        "    eval_loss = eval_loss / len(valid_dataloader)\n",
        "    validation_loss_values.append(eval_loss)\n",
        "    print(\"\\nValidation loss: {}\".format(eval_loss))\n",
        "\n",
        "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
        "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"[PAD]\"]\n",
        "    valid_tags = [tag_values[l_i] for l in true_labels\n",
        "                                  for l_i in l if tag_values[l_i] != \"[PAD]\"]\n",
        "    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
        "    print(\"---\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/3 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.240541756011429\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  33%|███▎      | 1/3 [03:51<07:42, 231.04s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation loss: 0.21366752803325653\n",
            "Validation Accuracy: 0.936051119476917\n",
            "---\n",
            "Average train loss: 0.19614867224322602\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [07:41<03:50, 230.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation loss: 0.2124370254576206\n",
            "Validation Accuracy: 0.9347879928670497\n",
            "---\n",
            "Average train loss: 0.16086946190303605\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 3/3 [11:33<00:00, 231.11s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation loss: 0.23559829637408256\n",
            "Validation Accuracy: 0.9340945115910442\n",
            "---\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km7xNJpnk1oU"
      },
      "source": [
        "## Get Model Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsPjeiVRcEH-",
        "outputId": "7da9254e-6dce-4204-85fb-0c104b896bd8"
      },
      "source": [
        "# В качестве теста - посмотрим, что у нас получилось на одном из предложений\n",
        "sent_id = 0\n",
        "\n",
        "# Токенизация предложения\n",
        "tokenized_sentence = tokenizer.encode(df.loc[sent_id].text_normalized)\n",
        "\n",
        "# Предикт\n",
        "input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
        "with torch.no_grad():\n",
        "    output = model(input_ids)\n",
        "\n",
        "# Приведение предикта в человеческий вид\n",
        "label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
        "\n",
        "# Вывод\n",
        "print('{0: <15} {1: <5} {2: <5}'.format(\"TOKEN\", \"PRED\", \"TRUE\"))\n",
        "print(\"-\"*27)\n",
        "for i, j, k in zip(tokens, label_indices[0], labels[sent_id]):\n",
        "    print('{0: <15} {1: <5} {2: <5}'.format(i, tag_values[j], k))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOKEN           PRED  TRUE \n",
            "---------------------------\n",
            "[CLS]           O     O    \n",
            "Another         O     O    \n",
            "violent         O     B-OFF\n",
            "and             I-OFF I-OFF\n",
            "aggressive      O     I-OFF\n",
            "immigrant       I-OFF I-OFF\n",
            "killing         I-OFF O    \n",
            "a               I-OFF O    \n",
            "innocent        I-OFF O    \n",
            "and             O     O    \n",
            "intelligent     O     O    \n",
            "US              O     O    \n",
            "Citizen         O     O    \n",
            ".               O     O    \n",
            ".               O     O    \n",
            ".               O     O    \n",
            ".               O     O    \n",
            "Sa              O     O    \n",
            "##rca           O     O    \n",
            "##sm            O     O    \n",
            "[SEP]           O     O    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwzscpTFiDBn",
        "outputId": "a30b9fa5-4ba0-4f77-f8f7-6c46d0448777"
      },
      "source": [
        "DEBUG = False\n",
        "\n",
        "for index, row in tqdm(df.iterrows()):\n",
        "    # Токенизация предложения\n",
        "    tokenized_sentence = tokenizer.encode(row.text_normalized)\n",
        "\n",
        "    # Предикт\n",
        "    input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids)\n",
        "\n",
        "    # Приведение предикта в человеческий вид\n",
        "    label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
        "\n",
        "    # Предложение, которое мы восстанавливаем\n",
        "    tmp = ''\n",
        "    # Маска, в которой мы отмечаем как 0 символы не токсичного промежутка, 1 - токсичного, _ - разделители, не являющиеся токенами\n",
        "    mask = ''\n",
        "    # Изначальное предложение, по которому мы проверяем качество восстановления\n",
        "    true = row.text\n",
        "\n",
        "    if DEBUG: print(true)\n",
        "\n",
        "    # Строим маску\n",
        "    for token, tag in zip(tokens, label_indices[0]):\n",
        "        if token in ['[CLS]', '[SEP]']:\n",
        "            continue\n",
        "        elif tmp == '':\n",
        "            tmp += token\n",
        "            if tag_values[tag] != 'O':\n",
        "                mask += '1' * len(token)\n",
        "            else:\n",
        "                mask += '0' * len(token)\n",
        "            true = true.replace(token, '', 1)\n",
        "        elif not true.startswith(token):\n",
        "            if token.startswith('##'):\n",
        "                tmp += token[2:]\n",
        "                true = true.replace(token[2:], '', 1)\n",
        "                if tag_values[tag] != 'O':\n",
        "                    mask += '1' * len(token[2:])\n",
        "                else:\n",
        "                    mask += '0' * len(token[2:])\n",
        "            else:\n",
        "                while not true.startswith(token) and true != '':\n",
        "                    tmp += true[0]\n",
        "                    true = true[1:]\n",
        "                    mask += '_'\n",
        "                tmp += token\n",
        "                true = true.replace(token, '', 1)\n",
        "                if tag_values[tag] != 'O':\n",
        "                    mask += '1' * len(token)\n",
        "                else:\n",
        "                    mask += '0' * len(token)\n",
        "\n",
        "    if DEBUG: print(tmp)\n",
        "    if DEBUG: print(mask)\n",
        "\n",
        "    # Восстанавливаем пробелы\n",
        "    _first = -1\n",
        "    _before = False\n",
        "    for i in range(len(mask)):\n",
        "        if mask[i] == '_' and mask[i-1]!='_':\n",
        "            _first = i\n",
        "            if mask[i-1] == '1':\n",
        "                _before = True\n",
        "            else:\n",
        "                _before = False\n",
        "        elif mask[i-1] == '_' and mask[i] != '_':\n",
        "            if _before and mask[i] == '1':\n",
        "                for j in range(_first, i):\n",
        "                    mask = mask[:j] + '1' + mask[j+1:]\n",
        "            else:\n",
        "                for j in range(_first, i):\n",
        "                    mask = mask[:j] + '0' + mask[j+1:]\n",
        "\n",
        "    if DEBUG: print(mask)\n",
        "    if DEBUG: print('---')\n",
        "\n",
        "    # Записываем pred\n",
        "    result = []\n",
        "    for i in range(len(mask)):\n",
        "        if mask[i] == '1':\n",
        "            result.append(i)\n",
        "    df.at[index, 'pred'] = str(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7939it [01:49, 72.38it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Edv8O1xv6_Ox",
        "outputId": "e85df58a-8845-4b37-b368-3a6695578876"
      },
      "source": [
        "for index, row in tqdm(df.iterrows()):\n",
        "    if row.spans == '[]' and row.pred == '[]':\n",
        "        df.at[index, 'score'] = 1\n",
        "    elif row.spans == '[]' and row.pred != '[]':\n",
        "        df.at[index, 'score'] = 0\n",
        "    elif row.spans != '[]' and row.pred == '[]':\n",
        "        df.at[index, 'score'] = 0\n",
        "    else:\n",
        "        arr_a = set([int(a) for a in row.pred[1:-1].split(', ')])\n",
        "        arr_g = set([int(a) for a in row.spans[1:-1].split(', ')])\n",
        "\n",
        "        p = len(arr_a & arr_g) / len(arr_a)\n",
        "        r = len(arr_a & arr_g) / len(arr_g)\n",
        "\n",
        "        df.at[index, 'score'] = (2 * p * r) / (p + r) if (p + r) != 0 else 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7939it [00:01, 5755.06it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbUsx4B_8zhh",
        "outputId": "c2886902-248a-4983-8a6c-24a4212049ca"
      },
      "source": [
        "print(f'Итоговый скор: {df.score.mean()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Итоговый скор: 0.6892408996963809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3xgG433D_uI"
      },
      "source": [
        "# BERT Lagre Cased Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yU3Wy3gEKUC"
      },
      "source": [
        "## Init CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Lr6raCoZ9zYT",
        "outputId": "8f26397c-cce4-4f25-8786-727aeaf3ce3a"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.7.0+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "K6wZbAXDENRo",
        "outputId": "07544666-a6f8-4b1b-a319-bed218fbcb95"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla V100-SXM2-16GB'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9002DNMkERiy"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We0VisBiEVuL"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "9LVidV0EEU9H",
        "outputId": "2d32f0e3-5268-4442-eeae-fd47081e8875"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('tsd_train.csv')\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spans</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,...</td>\n",
              "      <td>Another violent and aggressive immigrant killi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[33, 34, 35, 36, 37, 38, 39]</td>\n",
              "      <td>I am 56 years old, I am not your fucking junio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0, 1, 2, 3]</td>\n",
              "      <td>Damn, a whole family. Sad indeed.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]</td>\n",
              "      <td>What a knucklehead. How can anyone not know th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[32, 33, 34, 35, 36, 37, 38]</td>\n",
              "      <td>\"who do you think should do the killing?\"\\n\\nA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[]</td>\n",
              "      <td>But, but, but, is NOT a defense.  It's not eve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 5...</td>\n",
              "      <td>Please people, stop using these silly, stupid ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[0, 1, 2, 3]</td>\n",
              "      <td>Dumb.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[49, 50, 51, 52, 53, 54, 147, 148, 149, 150, 1...</td>\n",
              "      <td>Obamacare is on it's last gasping breaths.   Y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[32, 33, 34, 35, 36, 37, 38, 39]</td>\n",
              "      <td>CROOKED Trump = GUILTY as hell.\\npathetic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               spans                                               text\n",
              "0  [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,...  Another violent and aggressive immigrant killi...\n",
              "1                       [33, 34, 35, 36, 37, 38, 39]  I am 56 years old, I am not your fucking junio...\n",
              "2                                       [0, 1, 2, 3]                  Damn, a whole family. Sad indeed.\n",
              "3          [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]  What a knucklehead. How can anyone not know th...\n",
              "4                       [32, 33, 34, 35, 36, 37, 38]  \"who do you think should do the killing?\"\\n\\nA...\n",
              "5                                                 []  But, but, but, is NOT a defense.  It's not eve...\n",
              "6  [39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 5...  Please people, stop using these silly, stupid ...\n",
              "7                                       [0, 1, 2, 3]                                              Dumb.\n",
              "8  [49, 50, 51, 52, 53, 54, 147, 148, 149, 150, 1...  Obamacare is on it's last gasping breaths.   Y...\n",
              "9                   [32, 33, 34, 35, 36, 37, 38, 39]          CROOKED Trump = GUILTY as hell.\\npathetic"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65,
          "referenced_widgets": [
            "01c3f86a23c34f2a9cd2b58cc44154d5",
            "168542a6a6f6488ead299f5b7eb0247a",
            "ee18733eefcc44659cf885ac90996d73",
            "fead42ad98974c4281cb1edb5c4bafaf",
            "741a72c1c846482fbb56b1b411834793",
            "958010b3d731490299cbd9f6fe0b0e31",
            "378da732cae44c489bc537d6c5f6348b",
            "d7f65baf04644006bd44edbc009065f2"
          ]
        },
        "id": "92j4UnLJEe9D",
        "outputId": "c10155d6-4015-4b16-86dd-f435ab3411e2"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-large-cased', do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01c3f86a23c34f2a9cd2b58cc44154d5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDYDAO2dEkKK"
      },
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def list_replace(search, replacement, text):\n",
        "    \"\"\"\n",
        "    Replaces all symbols of text which are present\n",
        "    in the search string with the replacement string.\n",
        "    \"\"\"\n",
        "    search = [el for el in search if el in text]\n",
        "    for c in search:\n",
        "        text = text.replace(c, replacement)\n",
        "    return text\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans the given sentence\n",
        "    \"\"\"\n",
        "    text = list_replace('\\u00AB\\u00BB\\u2039\\u203A\\u201E\\u201A\\u201C\\u201F\\u2018\\u201B\\u201D\\u2019', '\\u0022', text)\n",
        "    text = list_replace('\\u2012\\u2013\\u2014\\u2015\\u203E\\u0305\\u00AF', '\\u2003\\u002D\\u002D\\u2003', text)\n",
        "    text = list_replace('\\u2010\\u2011', '\\u002D', text)\n",
        "    text = list_replace('\\u2000\\u2001\\u2002\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200A\\u200B\\u202F\\u205F\\u2060\\u3000', '\\u2002', text)\n",
        "    text = re.sub('\\u2003\\u2003', '\\u2003', text)\n",
        "    text = re.sub('\\t\\t', '\\t', text)\n",
        "    text = list_replace('\\u02CC\\u0307\\u0323\\u2022\\u2023\\u2043\\u204C\\u204D\\u2219\\u25E6\\u00B7\\u00D7\\u22C5\\u2219\\u2062', '.', text)\n",
        "    text = list_replace('\\u2217', '\\u002A', text)\n",
        "    text = list_replace('\\u00C4', 'A', text)\n",
        "    text = list_replace('\\u00E4', 'a', text)\n",
        "    text = list_replace('\\u00CB', 'E', text)\n",
        "    text = list_replace('\\u00EB', 'e', text)\n",
        "    text = list_replace('\\u1E26', 'H', text)\n",
        "    text = list_replace('\\u1E27', 'h', text)\n",
        "    text = list_replace('\\u00CF', 'I', text)\n",
        "    text = list_replace('\\u00EF', 'i', text)\n",
        "    text = list_replace('\\u00D6', 'O', text)\n",
        "    text = list_replace('\\u00F6', 'o', text)\n",
        "    text = list_replace('\\u00DC', 'U', text)\n",
        "    text = list_replace('\\u00FC', 'u', text)\n",
        "    text = list_replace('\\u0178', 'Y', text)\n",
        "    text = list_replace('\\u00FF', 'y', text)\n",
        "    text = list_replace('\\u00DF', 's', text)\n",
        "    text = list_replace('\\u1E9E', 'S', text)\n",
        "\n",
        "    # Replacing all numbers with masks\n",
        "    text = list_replace('0123456789', 'x', text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "bmEOAwDHEqoo",
        "outputId": "21a96f06-106d-419d-e93f-49471490894c"
      },
      "source": [
        "df['text_normalized'] = df.text.apply(clean_text)\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spans</th>\n",
              "      <th>text</th>\n",
              "      <th>text_normalized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,...</td>\n",
              "      <td>Another violent and aggressive immigrant killi...</td>\n",
              "      <td>Another violent and aggressive immigrant killi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[33, 34, 35, 36, 37, 38, 39]</td>\n",
              "      <td>I am 56 years old, I am not your fucking junio...</td>\n",
              "      <td>I am xx years old, I am not your fucking junio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0, 1, 2, 3]</td>\n",
              "      <td>Damn, a whole family. Sad indeed.</td>\n",
              "      <td>Damn, a whole family. Sad indeed.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]</td>\n",
              "      <td>What a knucklehead. How can anyone not know th...</td>\n",
              "      <td>What a knucklehead. How can anyone not know th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[32, 33, 34, 35, 36, 37, 38]</td>\n",
              "      <td>\"who do you think should do the killing?\"\\n\\nA...</td>\n",
              "      <td>\"who do you think should do the killing?\"\\n\\nA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[]</td>\n",
              "      <td>But, but, but, is NOT a defense.  It's not eve...</td>\n",
              "      <td>But, but, but, is NOT a defense.  It's not eve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 5...</td>\n",
              "      <td>Please people, stop using these silly, stupid ...</td>\n",
              "      <td>Please people, stop using these silly, stupid ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[0, 1, 2, 3]</td>\n",
              "      <td>Dumb.</td>\n",
              "      <td>Dumb.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[49, 50, 51, 52, 53, 54, 147, 148, 149, 150, 1...</td>\n",
              "      <td>Obamacare is on it's last gasping breaths.   Y...</td>\n",
              "      <td>Obamacare is on it's last gasping breaths.   Y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[32, 33, 34, 35, 36, 37, 38, 39]</td>\n",
              "      <td>CROOKED Trump = GUILTY as hell.\\npathetic</td>\n",
              "      <td>CROOKED Trump = GUILTY as hell.\\npathetic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               spans  ...                                    text_normalized\n",
              "0  [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,...  ...  Another violent and aggressive immigrant killi...\n",
              "1                       [33, 34, 35, 36, 37, 38, 39]  ...  I am xx years old, I am not your fucking junio...\n",
              "2                                       [0, 1, 2, 3]  ...                  Damn, a whole family. Sad indeed.\n",
              "3          [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]  ...  What a knucklehead. How can anyone not know th...\n",
              "4                       [32, 33, 34, 35, 36, 37, 38]  ...  \"who do you think should do the killing?\"\\n\\nA...\n",
              "5                                                 []  ...  But, but, but, is NOT a defense.  It's not eve...\n",
              "6  [39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 5...  ...  Please people, stop using these silly, stupid ...\n",
              "7                                       [0, 1, 2, 3]  ...                                              Dumb.\n",
              "8  [49, 50, 51, 52, 53, 54, 147, 148, 149, 150, 1...  ...  Obamacare is on it's last gasping breaths.   Y...\n",
              "9                   [32, 33, 34, 35, 36, 37, 38, 39]  ...          CROOKED Trump = GUILTY as hell.\\npathetic\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbXSMwV_EtqH"
      },
      "source": [
        "def create_labels(label_chars, text, embed, debug=False):\n",
        "    \"\"\"\n",
        "    Creates tokens and labels for BERT to work with\n",
        "    \"\"\"\n",
        "    res_tokens = []\n",
        "    res_labels = []\n",
        "\n",
        "    res_tokens.append('[CLS]')\n",
        "    res_labels.append('O')\n",
        "\n",
        "    if label_chars == '[]':\n",
        "        for i in embed:\n",
        "            res_tokens.append(i)\n",
        "            res_labels.append('O')\n",
        "    else:\n",
        "        _last = -1\n",
        "        _first = -1\n",
        "        arr = [int(a) for a in label_chars[1:-1].split(', ')]\n",
        "\n",
        "        bad_tokens = []\n",
        "        \n",
        "        for i in range(len(arr)):\n",
        "            if i == 0:\n",
        "                _first = arr[i]\n",
        "            elif arr[i-1] + 1 != arr[i]:\n",
        "                _last = arr[i-1]\n",
        "                _word = tokenizer.tokenize(text[_first:_last+1])\n",
        "                if debug: print(arr, _word, text)\n",
        "                bad_tokens.extend(_word)\n",
        "                _first = arr[i]\n",
        "            elif i == len(arr)-1:\n",
        "                _last = arr[i]\n",
        "                _word = tokenizer.tokenize(text[_first:_last+1])\n",
        "                if debug: print(arr, _word, text)\n",
        "                bad_tokens.extend(_word)\n",
        "        \n",
        "        j = 0\n",
        "        for i in embed:\n",
        "            if j < len(bad_tokens) and i == bad_tokens[j]:\n",
        "                j += 1\n",
        "                res_tokens.append(i)\n",
        "                res_labels.append('I-OFF')\n",
        "            else:\n",
        "                res_tokens.append(i)\n",
        "                res_labels.append('O')\n",
        "\n",
        "    res_tokens.append('[SEP]')\n",
        "    res_labels.append('O')\n",
        "\n",
        "    flag_isFirst = True\n",
        "    for i in range(len(res_labels)):\n",
        "        if res_labels[i] == 'I-OFF' and flag_isFirst:\n",
        "            res_labels[i] = 'B-OFF'\n",
        "            flag_isFirst = False\n",
        "        elif res_labels[i] == 'O':\n",
        "            flag_isFirst = True\n",
        "\n",
        "    if debug:\n",
        "        for i, j in zip(res_tokens, res_labels):\n",
        "            print(f'{i}\\t\\t\\t{j}')\n",
        "\n",
        "    return res_tokens, res_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umDe-4oaEx2l"
      },
      "source": [
        "tokenized_texts_and_labels = [\n",
        "    create_labels(span, text, tokenizer.tokenize(norm)) for span, text, norm in zip(df.spans.tolist(), df.text.tolist(), df.text_normalized.tolist())\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6aSfZMgE2sK"
      },
      "source": [
        "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
        "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jb8MxBKE7vb",
        "outputId": "fdd5e406-37f0-4ed9-b369-1509eb718bd3"
      },
      "source": [
        "print(f'Длина самого большого embeding: {max(list(map(len, tokenized_texts)))}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Длина самого большого embeding: 501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESvCUECdE_LU"
      },
      "source": [
        "MAX_LEN = 510\n",
        "BATCH_SIZE = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk3Acxd_FD98"
      },
      "source": [
        "input_ids = pad_sequences(\n",
        "    [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "    maxlen=MAX_LEN,\n",
        "    dtype=\"long\",\n",
        "    value=0.0,\n",
        "    truncating=\"post\",\n",
        "    padding=\"post\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB35s0UtFHbX"
      },
      "source": [
        "tag_values = ['O', '[PAD]', 'B-OFF', 'I-OFF']\n",
        "tag2idx = {t: i for i, t in enumerate(tag_values)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS44gjZVFKv5"
      },
      "source": [
        "tags = pad_sequences(\n",
        "    [[tag2idx.get(l) for l in lab] for lab in labels],\n",
        "    maxlen=MAX_LEN,\n",
        "    value=tag2idx[\"[PAD]\"],\n",
        "    padding=\"post\",\n",
        "    dtype=\"long\",\n",
        "    truncating=\"post\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AemkBwsCFPFn"
      },
      "source": [
        "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e4hop2fFUBU"
      },
      "source": [
        "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, random_state=42, test_size=0.1)\n",
        "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=42, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4RzDRGnFZQP"
      },
      "source": [
        "tr_inputs = torch.tensor(tr_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "tr_tags = torch.tensor(tr_tags)\n",
        "val_tags = torch.tensor(val_tags)\n",
        "tr_masks = torch.tensor(tr_masks)\n",
        "val_masks = torch.tensor(val_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-nXDbNEFgx-"
      },
      "source": [
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw1UtUqeFiIV"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "hXb9sOf-FhKR",
        "outputId": "860aff88-f573-4fe6-b260-ace71175a4e1"
      },
      "source": [
        "import numpy as np\n",
        "import transformers\n",
        "\n",
        "from tqdm import trange, tqdm\n",
        "from seqeval.metrics import f1_score, accuracy_score\n",
        "from transformers import BertForTokenClassification, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "transformers.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114,
          "referenced_widgets": [
            "6ee592945375443397d923a472b71989",
            "8551949750b14a909cab9a17aac15efb",
            "797694c8aeee4155b486365eb9465070",
            "d732410c46534276ac937bb10906fab0",
            "9f9e0639067b4c27a4dd156fd6152f7d",
            "7af8cf7b19b9416ba79b87457d6b8b90",
            "54bb45cfc33348f48a4eed60c39dfa14",
            "efb02a734c1d4483aa4efdb6ae770178",
            "b2188b5a67b04b7bbe9df91a3603755d",
            "270cd69c2e2d43deab02682a1d979eb3",
            "a179c9df0450469097236bce65a6f7a1",
            "2eca0a90d54c433b84f56ec4fab7064f",
            "b68be9c566024d959dd3d1c01bb1e347",
            "ab01b75f80af448ea39e6d27af5d43a5",
            "86181ab2d34c4b6a848cbd8b940939d8",
            "7b4d690569bb4d99b036dcd744aa6d11"
          ]
        },
        "id": "glqB-LxNFt4G",
        "outputId": "86a107b9-c0a6-4a81-c427-0328b78b04d8"
      },
      "source": [
        "model = BertForTokenClassification.from_pretrained(\n",
        "    \"bert-large-cased\",\n",
        "    num_labels=len(tag2idx),\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ee592945375443397d923a472b71989",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2188b5a67b04b7bbe9df91a3603755d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1338740706.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq2eN4WQFzxX"
      },
      "source": [
        "model.cuda();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOd9jrHCGDBs"
      },
      "source": [
        "FULL_FINETUNING = True\n",
        "\n",
        "if FULL_FINETUNING:\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "            'weight_decay_rate': 0.01\n",
        "        },\n",
        "        {\n",
        "            'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "            'weight_decay_rate': 0.0\n",
        "        }\n",
        "    ]\n",
        "else:\n",
        "    param_optimizer = list(model.classifier.named_parameters())\n",
        "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters,\n",
        "    lr=3e-5,\n",
        "    eps=1e-8\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTeXdrYHGHNf"
      },
      "source": [
        "EPOCHS_NUM = 3\n",
        "MAX_GRAD_NORM = 1.0\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * EPOCHS_NUM\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nK8r93e0GH99",
        "outputId": "3dbee7b0-ff54-4be2-87ac-4d6110d7be45"
      },
      "source": [
        "loss_values, validation_loss_values = [], []\n",
        "\n",
        "for _ in trange(EPOCHS_NUM, desc=\"Epoch\"):\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Training loop\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # Always clear any previously calculated gradients before performing a backward pass.\n",
        "        model.zero_grad()\n",
        "        # forward pass\n",
        "        # This will return the loss (rather than the model output)\n",
        "        # because we have provided the `labels`.\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        # get the loss\n",
        "        loss = outputs[0]\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        # track train loss\n",
        "        total_loss += loss.item()\n",
        "        # Clip the norm of the gradient\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_GRAD_NORM)\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    # Put the model into evaluation mode\n",
        "    model.eval()\n",
        "    # Reset the validation loss for this epoch.\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions , true_labels = [], []\n",
        "    for batch in valid_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients,\n",
        "        # saving memory and speeding up validation\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have not provided labels.\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        # Move logits and labels to CPU\n",
        "        logits = outputs[1].detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        eval_loss += outputs[0].mean().item()\n",
        "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "        true_labels.extend(label_ids)\n",
        "\n",
        "    eval_loss = eval_loss / len(valid_dataloader)\n",
        "    validation_loss_values.append(eval_loss)\n",
        "    print(\"\\nValidation loss: {}\".format(eval_loss))\n",
        "\n",
        "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
        "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"[PAD]\"]\n",
        "    valid_tags = [tag_values[l_i] for l in true_labels\n",
        "                                  for l_i in l if tag_values[l_i] != \"[PAD]\"]\n",
        "    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
        "    print(\"---\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/3 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.24870469009792448\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  33%|███▎      | 1/3 [14:22<28:45, 862.88s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation loss: 0.23953121398861085\n",
            "Validation Accuracy: 0.9360758866653458\n",
            "---\n",
            "Average train loss: 0.20286594124596355\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [28:45<14:22, 862.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation loss: 0.2350809817768671\n",
            "Validation Accuracy: 0.9371408757677828\n",
            "---\n",
            "Average train loss: 0.13105480274744227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 3/3 [43:07<00:00, 862.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation loss: 0.3371578547050568\n",
            "Validation Accuracy: 0.9293144442242917\n",
            "---\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKQOrFMNGsqj"
      },
      "source": [
        "## Get Model Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYYT1ztiGLhW",
        "outputId": "916e0bef-7c9e-4e27-9264-21ac8f014622"
      },
      "source": [
        "# В качестве теста - посмотрим, что у нас получилось на одном из предложений\n",
        "sent_id = 0\n",
        "\n",
        "# Токенизация предложения\n",
        "tokenized_sentence = tokenizer.encode(df.loc[sent_id].text_normalized)\n",
        "\n",
        "# Предикт\n",
        "input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
        "with torch.no_grad():\n",
        "    output = model(input_ids)\n",
        "\n",
        "# Приведение предикта в человеческий вид\n",
        "label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
        "\n",
        "# Вывод\n",
        "print('{0: <15} {1: <5} {2: <5}'.format(\"TOKEN\", \"PRED\", \"TRUE\"))\n",
        "print(\"-\"*27)\n",
        "for i, j, k in zip(tokens, label_indices[0], labels[sent_id]):\n",
        "    print('{0: <15} {1: <5} {2: <5}'.format(i, tag_values[j], k))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOKEN           PRED  TRUE \n",
            "---------------------------\n",
            "[CLS]           O     O    \n",
            "Another         O     O    \n",
            "violent         B-OFF B-OFF\n",
            "and             I-OFF I-OFF\n",
            "aggressive      I-OFF I-OFF\n",
            "immigrant       I-OFF I-OFF\n",
            "killing         O     O    \n",
            "a               O     O    \n",
            "innocent        O     O    \n",
            "and             O     O    \n",
            "intelligent     O     O    \n",
            "US              O     O    \n",
            "Citizen         O     O    \n",
            ".               O     O    \n",
            ".               O     O    \n",
            ".               O     O    \n",
            ".               O     O    \n",
            "Sa              O     O    \n",
            "##rca           O     O    \n",
            "##sm            O     O    \n",
            "[SEP]           O     O    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7mgnIZwGy1a",
        "outputId": "9c9b2d8d-b65d-4187-ddcd-169dcaff15a9"
      },
      "source": [
        "DEBUG = False\n",
        "\n",
        "for index, row in tqdm(df.iterrows()):\n",
        "    # Токенизация предложения\n",
        "    tokenized_sentence = tokenizer.encode(row.text_normalized)\n",
        "\n",
        "    # Предикт\n",
        "    input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids)\n",
        "\n",
        "    # Приведение предикта в человеческий вид\n",
        "    label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
        "\n",
        "    # Предложение, которое мы восстанавливаем\n",
        "    tmp = ''\n",
        "    # Маска, в которой мы отмечаем как 0 символы не токсичного промежутка, 1 - токсичного, _ - разделители, не являющиеся токенами\n",
        "    mask = ''\n",
        "    # Изначальное предложение, по которому мы проверяем качество восстановления\n",
        "    true = row.text\n",
        "\n",
        "    if DEBUG: print(true)\n",
        "\n",
        "    # Строим маску\n",
        "    for token, tag in zip(tokens, label_indices[0]):\n",
        "        if token in ['[CLS]', '[SEP]']:\n",
        "            continue\n",
        "        elif tmp == '':\n",
        "            tmp += token\n",
        "            if tag_values[tag] != 'O':\n",
        "                mask += '1' * len(token)\n",
        "            else:\n",
        "                mask += '0' * len(token)\n",
        "            true = true.replace(token, '', 1)\n",
        "        elif not true.startswith(token):\n",
        "            if token.startswith('##'):\n",
        "                tmp += token[2:]\n",
        "                true = true.replace(token[2:], '', 1)\n",
        "                if tag_values[tag] != 'O':\n",
        "                    mask += '1' * len(token[2:])\n",
        "                else:\n",
        "                    mask += '0' * len(token[2:])\n",
        "            else:\n",
        "                while not true.startswith(token) and true != '':\n",
        "                    tmp += true[0]\n",
        "                    true = true[1:]\n",
        "                    mask += '_'\n",
        "                tmp += token\n",
        "                true = true.replace(token, '', 1)\n",
        "                if tag_values[tag] != 'O':\n",
        "                    mask += '1' * len(token)\n",
        "                else:\n",
        "                    mask += '0' * len(token)\n",
        "\n",
        "    if DEBUG: print(tmp)\n",
        "    if DEBUG: print(mask)\n",
        "\n",
        "    # Восстанавливаем пробелы\n",
        "    _first = -1\n",
        "    _before = False\n",
        "    for i in range(len(mask)):\n",
        "        if mask[i] == '_' and mask[i-1]!='_':\n",
        "            _first = i\n",
        "            if mask[i-1] == '1':\n",
        "                _before = True\n",
        "            else:\n",
        "                _before = False\n",
        "        elif mask[i-1] == '_' and mask[i] != '_':\n",
        "            if _before and mask[i] == '1':\n",
        "                for j in range(_first, i):\n",
        "                    mask = mask[:j] + '1' + mask[j+1:]\n",
        "            else:\n",
        "                for j in range(_first, i):\n",
        "                    mask = mask[:j] + '0' + mask[j+1:]\n",
        "\n",
        "    if DEBUG: print(mask)\n",
        "    if DEBUG: print('---')\n",
        "\n",
        "    # Записываем pred\n",
        "    result = []\n",
        "    for i in range(len(mask)):\n",
        "        if mask[i] == '1':\n",
        "            result.append(i)\n",
        "    df.at[index, 'pred'] = str(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7939it [03:20, 39.63it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4z0uBnPG-k8",
        "outputId": "c5ee9e03-8188-4675-f2f5-bf6477b85f6f"
      },
      "source": [
        "for index, row in tqdm(df.iterrows()):\n",
        "    if row.spans == '[]' and row.pred == '[]':\n",
        "        df.at[index, 'score'] = 1\n",
        "    elif row.spans == '[]' and row.pred != '[]':\n",
        "        df.at[index, 'score'] = 0\n",
        "    elif row.spans != '[]' and row.pred == '[]':\n",
        "        df.at[index, 'score'] = 0\n",
        "    else:\n",
        "        arr_a = set([int(a) for a in row.pred[1:-1].split(', ')])\n",
        "        arr_g = set([int(a) for a in row.spans[1:-1].split(', ')])\n",
        "\n",
        "        p = len(arr_a & arr_g) / len(arr_a)\n",
        "        r = len(arr_a & arr_g) / len(arr_g)\n",
        "\n",
        "        df.at[index, 'score'] = (2 * p * r) / (p + r) if (p + r) != 0 else 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7939it [00:01, 5855.42it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz1QsjnHHEIx",
        "outputId": "d93ab241-8b30-4cdc-b69a-984d0077ab7f"
      },
      "source": [
        "print(f'Итоговый скор: {df.score.mean()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Итоговый скор: 0.7626692466158488\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}